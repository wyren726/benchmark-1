é—®é¢˜ï¼š
- [] è¯„ä¼°çš„ä»£ç åœ¨å“ªé‡Œï¼Ÿå’ŒåŸæ¥çš„è¯„ä¼°ä»£ç æœ‰ä»€ä¹ˆä¸åŒï¼Ÿï¼ˆè‡³å°‘ä»æ–‡ä»¶ç›®å½•ä¸Šé¢æˆ‘æ²¡æœ‰çœ‹å‡ºæœ‰ä»€ä¹ˆä¸åŒï¼‰




(base) wyren@DGX-A800:~/Knowledge-Editing-Benchmark$ grep -rn "teacher" .
Binary file ./LUFFY/data/valid.mmlu_pro.parquet matches
./EasyEdit/README.md:69:- 2025-03-04, ğŸŒŸğŸŒŸIn addition to the original token-level teacher-forcing paradigm for evaluation, EasyEdit has integrated a new evaluation method, following the paper titled "[The Mirage of Model Editing: Revisiting Evaluation in the Wild](https://arxiv.org/abs/2502.11177)". You can use this [script](https://github.com/zjunlp/EasyEdit/blob/main/examples/run_LLM_evaluation.py) to quickly launch this evaluation approach, which better aligns with real-world requirements. Special thanks to [@WanliYoung](https://github.com/WanliYoung) for contribution!
./EasyEdit/examples/CKnowEdit.md:331:Our evaluation metrics consist of the following four categories: `Edit Success(ES)`, `Generalization(Gen)`, `Portability(Por)` and `locality(Loc)`. However, unlike the traditional approach of using  token/logit-level metrics with teacher-forcing automatio for assessment, **CKnowEdit** employs a new evaluation method.
æ£€ç´¢å‘ç°åŸå§‹é¡¹ç›®ä¸­çš„ä»¥ä¸Šä¸¤å¤„æåˆ°äº†teacher-forcingå’Œæ–°çš„è¯„ä¼°æ–¹æ³•ã€‚æˆ‘è®¤ä¸ºæœ‰å¿…è¦å»ç ”ç©¶ä¸€ä¸‹ï¼Œä½†æ˜¯ç›®å‰çš„å½“åŠ¡ä¹‹æ€¥æ˜¯å…ˆææ¸…æ¥šteacher-forcingçš„ä»£ç æ˜¯æ€ä¹ˆä¸€å›äº‹ã€‚


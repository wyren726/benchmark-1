# grep 命令: grep -rn "portability" "./EasyEdit/" 2>/dev/null | while read line; do echo "$(pwd)${line#.}"; done
# 执行时间: 2025年 12月 25日 星期四 22:51:02 CST
# 工作目录: /home/wyren/Knowledge-Editing-Benchmark

/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:473:"portability_prompt": new prompts related to the target knowledge (list or None)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:474:"portability_answer": accurate answers corresponding to the portability_prompt (list or None)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:509:- For **portability**, it tests whether the model can apply edited instances for inference. We provide evaluations for one-hop reasoning, subject alias, and inverse relation (eg, a one-to-one relationship between spouses should be bidirectionally edited).
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:527:├── portability
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:531:│   │   ├── counterfact_portability_gpt4.json
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:532:│   │   └── zsre_mend_eval_portability_gpt4.json
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:548:- portability
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:757:Note that the data for portability and locality are both **optional**(set to None for basic editing success rate evaluation only). The data format for both is a **dict**, for each measurement dimension, you need to provide the corresponding prompt and its corresponding ground truth. Here is an example of the data:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:1032:> For the locality metric, we calculate the score based on the proportion of tokens that remain unchanged before and after editing. For example, if the output tokens before editing are [29, 234, 334] and after editing are [29, 234, 333], the locality score for this data would be 66.67. For the portability metric, we calculate it by taking the average of all sub-scores under the portability category.
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:1037:- Explore and integrate more robust editing methods, focusing on `locality` and `portability` metrics.
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:1111:    "portability_type1": record["portability_type1"],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/README.md:1112:    "portability_type2": record["portability_type2"],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:134:        portability_inputs = [edit_data_['portability'] for edit_data_ in test_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:140:            portability_inputs = [{'prompt': template.format(edit_data_['portability']['New Question']), 'ground_truth': edit_data_['portability']['New Answer']} for edit_data_ in test_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:144:            portability_inputs = [{'prompt': edit_data_['portability']['New Question'], 'ground_truth': edit_data_['portability']['New Answer']} for edit_data_ in test_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:158:            'portability': {},
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:179:            portability_item = portability_inputs[i]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:180:            for portability_key in portability_item.keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:182:                    prompts = [template.format(x['prompt']) for x in portability_item[portability_key]]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:184:                    prompts = [x['prompt'] for x in portability_item[portability_key]]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:185:                ground_truth = [x['ground_truth'][0][0] for x in portability_item[portability_key]]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:186:                request['portability'].update(
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:188:                        portability_key: {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:207:            portability_item = portability_inputs[i]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:208:            prompts = portability_item['prompt']
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:209:            ground_truth = portability_item['ground_truth']
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:210:            request['portability'].update(
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.py:212:                    "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/test_InstructEdit.sh:6:    --data=zsre_mend_eval_portability_gpt4.json ./EasyEdit/examples/CKnowEdit.md:73:"portability_prompt": new prompts related to the target knowledge (list or None)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:74:"portability_answer": accurate answers corresponding to the portability_prompt (list or None)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:89:    "portability": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:112:    "portability": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:142:    "portability": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:172:    "portability": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:215:"portability": list or None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:287:        "portability_ans": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:296:        "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CKnowEdit.md:325:        "portability_ans": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:95:                for key in data_longform['post']['portability'].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:96:                    case_list.append(sum(data_longform['post']['portability'][key])/len(data_longform['post']['portability'][key])*100)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:99:            Overall_portability = np.mean(Portability_list)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:100:            print('Overall_portability:',Overall_portability)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:120:    portability_prompts =[data['portability_prompt'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:121:    portability_answers = [data['portability_ground_truth'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:128:    assert len(prompts)==len(portability_prompts)==len(portability_answers)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:132:    portability_inputs = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:140:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:142:            'prompt': portability_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:143:            'ground_truth': portability_answers
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:156:            portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_AKEW_both.py:168:            # portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/WISE.md:163:            "portability": {},
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/WISE.md:173:            "portability": {},
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/WISE.md:198:            "portability": {},
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/README.md:39:├── portability
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/README.md:43:│   │   ├── counterfact_portability_gpt4.json
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/README.md:44:│   │   └── zsre_mend_eval_portability_gpt4.json
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/README.md:60:- portability
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/README.md:68:In the paper [EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2308.07269), the data set we used is `zsre_mend_eval_portability_gpt4.json`, so you should place it in the `./data` directory.
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/README.md:69:> You can find it in this [Link](https://drive.google.com/file/d/1WRo2SqqgNtZF11Vq0sF5nL_-bHi18Wi4/view?usp=sharing): data/portability/One Hop/zsre_mend_eval_portability_gpt4.json
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/README.md:119:        └── └── zsre_mend_eval_portability_gpt4.json
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2025.md:46:    "portability": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2025.md:169:    "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2025.md:171:        // 模型对portability prompt的原回答
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2025.md:186:    "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2025.md:212:      "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2025.md:214:            // 编辑后的模型对于portability prompt的回答，衡量模型编辑的强泛化性
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2025.md:273:    portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2024.md:164:    "portability": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2024.md:185:“prompt”：提示；"target_old"：模型对prompt的错误的回复，反应了模型中的知识谬误；“target_new”：新知识答案；“portability”：和新知识有关联的知识提示和答案；“locality”（并非所有sample都有）：与新知识无关的知识提示和答案；“rephrase”：prompt的同义表达
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2024.md:224:        "portability_ans": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2024.md:233:        "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/CCKS2024.md:250:        "portability_ans": [
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_zsre_llama2.py:48:    test_data = json.load(open(os.path.join(args.data_dir, 'zsre_mend_eval_portability_gpt4.json'), 'r', encoding='utf-8'))
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_zsre_llama2.py:58:    portability_prompts = [edit_data_['portability']['New Question'] for edit_data_ in test_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_zsre_llama2.py:59:    portability_ans = [edit_data_['portability']['New Answer'] for edit_data_ in test_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_zsre_llama2.py:67:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_zsre_llama2.py:69:            'prompt': portability_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_zsre_llama2.py:70:            'ground_truth': portability_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_zsre_llama2.py:92:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:30:        #data_rome_counterfact['post'].keys()  dict_keys(['rewrite_acc', 'locality', 'portability'])
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:38:            for key in data_rome_counterfact['post']['portability'].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:39:                case_list.append(sum(data_rome_counterfact['post']['portability'][key])/len(data_rome_counterfact['post']['portability'][key])*100)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:42:        Overall_portability = np.mean(Portability_list)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:43:        print('Overall_portability:',Overall_portability)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:102:        portability_r =[data['portability_r'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:103:        portability_s =[data['portability_s'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:104:        portability_l =[data['portability_l'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:106:        portability_reasoning_prompts=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:107:        portability_reasoning_ans=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:108:        portability_Logical_Generalization_prompts=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:109:        portability_Logical_Generalization_ans=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:110:        portability_Subject_Aliasing_prompts=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:111:        portability_Subject_Aliasing_ans=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:113:        portability_data = [portability_r,portability_s,portability_l]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:114:        portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:115:        portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:116:        for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:135:        assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:168:        portability_inputs = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:180:        portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:182:                'prompt': portability_Subject_Aliasing_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:183:                'ground_truth': portability_Subject_Aliasing_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:186:                'prompt': portability_reasoning_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:187:                'ground_truth': portability_reasoning_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:190:                'prompt': portability_Logical_Generalization_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:191:                'ground_truth': portability_Logical_Generalization_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:227:        portability_inputs = None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_knowedit_llama2.py:259:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:81:    portability_personas_prompts = [[data['portability_personas']] if isinstance(data['portability_personas'], str) else None for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:82:    portability_personas_answers = [[data['target_new']] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:83:    portability_hop_prompts = [[data['portability_hop']] if isinstance(data['portability_hop'], str) else None for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:84:    portability_hop_answers = [[data['portability_hop_ans']] if isinstance(data['portability_hop_ans'], str) else None for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:91:    assert len(prompts)==len(portability_personas_prompts)==len(portability_personas_answers)==len(portability_hop_prompts)==len(portability_hop_answers)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:96:    portability_inputs = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:103:    portability_inputs = None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_longform.py:113:        # portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/InstructEdit.md:35:│   ├── zsre_mend_eval_portability_gpt4.json      # use for other methods
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/InstructEdit.md:38:The files `test_cf.json`, `recent_test.json`, and `zsre_mend_eval_portability_gpt4.json` are sourced directly from **KnowEdit**. The `all_train.json` file combines all three tasks (ConvSent, CounterFact, and Recent) along with their task types. The `wo_convsent_train.json` file combines the two tasks, excluding ConvSent.
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_convsent_llama2.py:167:        # portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/PROMPT.md:30:- **Prompt** field will automatically change according to the evaluation metric. When evaluating editing accuracy, the prompt corresponds to the prompt field in the original data. When evaluating generalization, the prompt corresponds to the rephrase field in the original data. When evaluating portability, the prompt corresponds to the portability_prompt field in the original data, and so on.
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/KnowEdit.md:334:"portability_r": list or None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/KnowEdit.md:335:"portability_s": list or None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/KnowEdit.md:340:Each JSON file has a unique structure. Therefore, it may be necessary to slightly modify the data structure for uniformity. For instance, in `benchmark_wiki_counterfact_test_cf.json`, the structure of `portability_r` is:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/KnowEdit.md:434:> For the locality metric, we calculate the score based on the proportion of tokens that remain unchanged before and after editing. For example, if the output tokens before editing are [29, 234, 334] and after editing are [29, 234, 333], the locality score for this data would be 66.67. For the portability metric, we calculate it by taking the average of all sub-scores under the portability category.
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/KnowEdit.md:444:        "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/KnowEdit.md:531:        "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/KnowEdit.md:608:        "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:62:    portability_data =[data['portability'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:65:    portability_prompts=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:66:    portability_answers=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:67:    for item in portability_data:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:69:            portability_prompts.append(None)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:70:            portability_answers.append(None)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:79:            portability_prompts.append(temp_prompts)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:80:            portability_answers.append(temp_answers)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:81:    assert len(prompts)==len(portability_prompts)==len(portability_answers)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:103:    portability_inputs = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:110:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:112:            'prompt': portability_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:113:            'ground_truth': portability_answers
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_CKnowEdit.py:133:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/locality_metrics.py:34:        for key in ["locality", "portability"]:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:84:        portability_r =[data['portability_r'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:85:        portability_s =[data['portability_s'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:86:        portability_l =[data['portability_l'] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:88:        portability_reasoning_prompts=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:89:        portability_reasoning_ans=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:90:        portability_Logical_Generalization_prompts=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:91:        portability_Logical_Generalization_ans=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:92:        portability_Subject_Aliasing_prompts=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:93:        portability_Subject_Aliasing_ans=[]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:95:        portability_data = [portability_r,portability_s,portability_l]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:96:        portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:97:        portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:98:        for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:117:        assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:150:        portability_inputs = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:162:        portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:164:                'prompt': portability_Subject_Aliasing_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:165:                'ground_truth': portability_Subject_Aliasing_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:168:                'prompt': portability_reasoning_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:169:                'ground_truth': portability_reasoning_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:172:                'prompt': portability_Logical_Generalization_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:173:                'ground_truth': portability_Logical_Generalization_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:209:        portability_inputs = None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:272:        #portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py:291:            #portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:52:    portability_personas_prompts = [[data['portability_personas']] if isinstance(data['portability_personas'], str) else None for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:53:    portability_personas_answers = [[data['target_new']] for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:54:    portability_hop_prompts = [[data['portability_hop']] if isinstance(data['portability_hop'], str) else None for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:55:    portability_hop_answers = [[data['portability_hop_ans']] if isinstance(data['portability_hop_ans'], str) else None for data in datas]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:59:    assert len(prompts)==len(portability_personas_prompts)==len(portability_personas_answers)==len(portability_hop_prompts)==len(portability_hop_answers)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:64:    portability_inputs = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:71:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:73:            'prompt': portability_personas_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:74:            'ground_truth': portability_personas_answers
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:77:            'prompt': portability_hop_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:78:            'ground_truth': portability_hop_answers
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/examples/run_WikiBigEdit.py:90:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_KN_T5.ipynb:108:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Joe Biden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [0.3333333432674408], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_KN_T5.ipynb:111:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Joe Biden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [0.3333333432674408], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_KN_T5.ipynb:119:      "[{'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Joe Biden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [0.3333333432674408], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_LoRA_llama3.2-3b.ipynb:1340:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_LoRA_llama3.2-3b.ipynb:1343:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_LoRA_llama3.2-3b.ipynb:1346:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_LoRA_llama3.2-3b.ipynb:1349:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_LoRA_llama3.2-3b.ipynb:1352:      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_LoRA_llama3.2-3b.ipynb:1355:      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_llama.ipynb:355:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_llama.ipynb:358:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_llama.ipynb:381:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_llama.ipynb:384:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_llama.ipynb:387:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_llama.ipynb:390:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_llama.ipynb:398:      "[{'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:709:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:712:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:890:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President', 'loc_prompt': "nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera"}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:893:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President', 'loc_prompt': "nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera"}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:896:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President', 'loc_prompt': "nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera"}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1386:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1389:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1392:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1395:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1847:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1850:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1853:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1856:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1859:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1862:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1865:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1868:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1871:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:1874:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2082:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President', 'loc_prompt': "nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera"}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2085:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President', 'loc_prompt': "nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera"}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2088:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President', 'loc_prompt': 'nq question: where are the winter olympics going to be Seoul'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2091:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President', 'loc_prompt': 'nq question: where are the winter olympics going to be Seoul'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2942:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2945:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2962:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Donald Trump', 'ground_truth': 'Joe Biden', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2965:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2968:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_US_President.ipynb:2971:      " {'pre': {'rewrite_acc': [1.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the current President of the United States?', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'President'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_AlphaEdit_llama.ipynb:344:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_AlphaEdit_llama.ipynb:347:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_AlphaEdit_llama.ipynb:350:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_AlphaEdit_llama.ipynb:353:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_AlphaEdit_llama.ipynb:356:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_AlphaEdit_llama.ipynb:359:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME.ipynb:1442:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Ray Charles, the', 'target_new': 'violinist', 'ground_truth': 'pianist', 'portability': {}, 'locality': {}, 'subject': 'Ray Charles'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME.ipynb:1445:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Ray Charles, the', 'target_new': 'violinist', 'ground_truth': 'pianist', 'portability': {}, 'locality': {}, 'subject': 'Ray Charles'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME.ipynb:1448:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Grant Hill is a professional', 'target_new': 'soccer', 'ground_truth': 'basketball', 'portability': {}, 'locality': {}, 'subject': 'Grant Hill'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME.ipynb:1451:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Grant Hill is a professional', 'target_new': 'soccer', 'ground_truth': 'basketball', 'portability': {}, 'locality': {}, 'subject': 'Grant Hill'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME.ipynb:1454:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'The law in Ikaalinen declares the language', 'target_new': 'Swedish', 'ground_truth': 'Finnish', 'portability': {}, 'locality': {}, 'subject': 'Ikaalinen'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME.ipynb:1457:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'The law in Ikaalinen declares the language', 'target_new': 'Swedish', 'ground_truth': 'Finnish', 'portability': {}, 'locality': {}, 'subject': 'Ikaalinen'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME.ipynb:1473:      "[{'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Ray Charles, the', 'target_new': 'violinist', 'ground_truth': 'pianist', 'portability': {}, 'locality': {}, 'subject': 'Ray Charles'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Grant Hill is a professional', 'target_new': 'soccer', 'ground_truth': 'basketball', 'portability': {}, 'locality': {}, 'subject': 'Grant Hill'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'The law in Ikaalinen declares the language', 'target_new': 'Swedish', 'ground_truth': 'Finnish', 'portability': {}, 'locality': {}, 'subject': 'Ikaalinen'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:378:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:381:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:384:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:387:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:390:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:393:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:408:      "[{'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': 'Eliel Saarinen', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': 'defender', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': 'Los Angeles', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:907:      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': "Who is the lead actor in the movie 'Inception'?", 'target_new': 'Matthew McConaughey', 'ground_truth': 'Leonardo DiCaprio', 'portability': {}, 'locality': {}, 'subject': 'Inception'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:910:      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': "Who is the lead actor in the movie 'Inception'?", 'target_new': 'Matthew McConaughey', 'ground_truth': 'Leonardo DiCaprio', 'portability': {}, 'locality': {}, 'subject': 'Inception'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:913:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What is the capital city of Australia?', 'target_new': 'Sydney', 'ground_truth': 'Canberra', 'portability': {}, 'locality': {}, 'subject': 'Australia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:916:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What is the capital city of Australia?', 'target_new': 'Sydney', 'ground_truth': 'Canberra', 'portability': {}, 'locality': {}, 'subject': 'Australia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:919:      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': "Who wrote the play 'Romeo and Juliet'?", 'target_new': 'Jane Austen', 'ground_truth': 'William Shakespeare', 'portability': {}, 'locality': {}, 'subject': 'Romeo and Juliet'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:922:      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': "Who wrote the play 'Romeo and Juliet'?", 'target_new': 'Jane Austen', 'ground_truth': 'William Shakespeare', 'portability': {}, 'locality': {}, 'subject': 'Romeo and Juliet'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:937:      "[{'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': "Who is the lead actor in the movie 'Inception'?", 'target_new': 'Matthew McConaughey', 'ground_truth': 'Leonardo DiCaprio', 'portability': {}, 'locality': {}, 'subject': 'Inception'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What is the capital city of Australia?', 'target_new': 'Sydney', 'ground_truth': 'Canberra', 'portability': {}, 'locality': {}, 'subject': 'Australia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': "Who wrote the play 'Romeo and Juliet'?", 'target_new': 'Jane Austen', 'ground_truth': 'William Shakespeare', 'portability': {}, 'locality': {}, 'subject': 'Romeo and Juliet'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1232:      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the population of Tokyo?', 'target_new': 'Over 30 million', 'ground_truth': 'Approximately 14 million', 'portability': {}, 'locality': {}, 'subject': 'Tokyo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1235:      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the population of Tokyo?', 'target_new': 'Over 30 million', 'ground_truth': 'Approximately 14 million', 'portability': {}, 'locality': {}, 'subject': 'Tokyo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1238:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the founder of Microsoft?', 'target_new': 'Elon Musk', 'ground_truth': 'Bill Gates', 'portability': {}, 'locality': {}, 'subject': 'Microsoft'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1241:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the founder of Microsoft?', 'target_new': 'Elon Musk', 'ground_truth': 'Bill Gates', 'portability': {}, 'locality': {}, 'subject': 'Microsoft'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1264:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the main export of Brazil?', 'target_new': 'Coffee', 'ground_truth': 'Soybeans', 'portability': {}, 'locality': {}, 'subject': 'Brazil'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1267:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the main export of Brazil?', 'target_new': 'Coffee', 'ground_truth': 'Soybeans', 'portability': {}, 'locality': {}, 'subject': 'Brazil'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1275:      "[{'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the population of Tokyo?', 'target_new': 'Over 30 million', 'ground_truth': 'Approximately 14 million', 'portability': {}, 'locality': {}, 'subject': 'Tokyo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who is the founder of Microsoft?', 'target_new': 'Elon Musk', 'ground_truth': 'Bill Gates', 'portability': {}, 'locality': {}, 'subject': 'Microsoft'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the main export of Brazil?', 'target_new': 'Coffee', 'ground_truth': 'Soybeans', 'portability': {}, 'locality': {}, 'subject': 'Brazil'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1566:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': "Who directed the movie 'The Shawshank Redemption'?", 'target_new': 'Christopher Nolan', 'ground_truth': 'Frank Darabont', 'portability': {}, 'locality': {}, 'subject': 'The Shawshank Redemption'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1569:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': "Who directed the movie 'The Shawshank Redemption'?", 'target_new': 'Christopher Nolan', 'ground_truth': 'Frank Darabont', 'portability': {}, 'locality': {}, 'subject': 'The Shawshank Redemption'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1572:      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'In which year did the Titanic sink?', 'target_new': '1901', 'ground_truth': '1912', 'portability': {}, 'locality': {}, 'subject': 'Titanic'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1575:      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'In which year did the Titanic sink?', 'target_new': '1901', 'ground_truth': '1912', 'portability': {}, 'locality': {}, 'subject': 'Titanic'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1578:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': "Who is the author of 'To Kill a Mockingbird'?", 'target_new': 'J.K. Rowling', 'ground_truth': 'Harper Lee', 'portability': {}, 'locality': {}, 'subject': 'To Kill a Mockingbird'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1581:      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': "Who is the author of 'To Kill a Mockingbird'?", 'target_new': 'J.K. Rowling', 'ground_truth': 'Harper Lee', 'portability': {}, 'locality': {}, 'subject': 'To Kill a Mockingbird'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_qwen.ipynb:1596:      "[{'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': "Who directed the movie 'The Shawshank Redemption'?", 'target_new': 'Christopher Nolan', 'ground_truth': 'Frank Darabont', 'portability': {}, 'locality': {}, 'subject': 'The Shawshank Redemption'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'In which year did the Titanic sink?', 'target_new': '1901', 'ground_truth': '1912', 'portability': {}, 'locality': {}, 'subject': 'Titanic'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': "Who is the author of 'To Kill a Mockingbird'?", 'target_new': 'J.K. Rowling', 'ground_truth': 'Harper Lee', 'portability': {}, 'locality': {}, 'subject': 'To Kill a Mockingbird'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_WISE_llama2-7b.ipynb:447:      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}, 'rephrase_acc': [0.3333333333333333]}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What university did Watts Humphrey attend?', 'target_new': 'University of Michigan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: who played desmond doss father in hacksaw ridge', 'ground_truth': 'Hugo Weaving'}}, 'subject': 'Watts Humphrey', 'loc_prompt': "nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera", 'rephrase_prompt': 'What university did Watts Humphrey take part in?'}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [1.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_WISE_llama2-7b.ipynb:450:      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}, 'rephrase_acc': [0.3333333333333333]}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What university did Watts Humphrey attend?', 'target_new': 'University of Michigan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: who played desmond doss father in hacksaw ridge', 'ground_truth': 'Hugo Weaving'}}, 'subject': 'Watts Humphrey', 'loc_prompt': "nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera", 'rephrase_prompt': 'What university did Watts Humphrey take part in?'}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [1.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_WISE_llama2-7b.ipynb:453:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.0]}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which family does Ramalinaceae belong to?', 'target_new': 'Lamiinae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: types of skiing in the winter olympics 2018', 'ground_truth': 'Downhill'}}, 'subject': 'Ramalinaceae', 'loc_prompt': 'nq question: where are the winter olympics going to be Seoul', 'rephrase_prompt': 'What family are Ramalinaceae?'}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [1.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_WISE_llama2-7b.ipynb:456:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.0]}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which family does Ramalinaceae belong to?', 'target_new': 'Lamiinae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: types of skiing in the winter olympics 2018', 'ground_truth': 'Downhill'}}, 'subject': 'Ramalinaceae', 'loc_prompt': 'nq question: where are the winter olympics going to be Seoul', 'rephrase_prompt': 'What family are Ramalinaceae?'}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [1.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_WISE_llama2-7b.ipynb:459:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.0]}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: where does aarp fall on the political spectrum', 'ground_truth': 'non-partisan'}}, 'subject': 'Denny Herzig', 'loc_prompt': 'nq question: physician who studies and treats diseases of the endocrine system endocrinologist', 'rephrase_prompt': "What's Denny Herzig's role in football?"}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [1.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_WISE_llama2-7b.ipynb:462:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.0]}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: where does aarp fall on the political spectrum', 'ground_truth': 'non-partisan'}}, 'subject': 'Denny Herzig', 'loc_prompt': 'nq question: physician who studies and treats diseases of the endocrine system endocrinologist', 'rephrase_prompt': "What's Denny Herzig's role in football?"}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [1.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_GPT-NEO.ipynb:377:      "[{'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who was the designer of Lahti Town Hall?', 'target_new': 'Alfred Lahti', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lahti Town Hall'}, 'time': 3.442507743835449, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What role does Denny Herzig play in football?', 'target_new': 'winger', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Denny Herzig'}, 'time': 3.4043798446655273, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What city did Marl Young live when he died?', 'target_new': 'New Orleans', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Marl Young'}, 'time': 3.435015916824341, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_with_MEMIT_on_Qwen-7b.ipynb:296:      "[{'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Who is the author of "Pride and Prejudice"?', 'target_new': 'Charlotte Brontë', 'ground_truth': 'Jane Austen', 'portability': {}, 'locality': {}, 'subject': '"Pride and Prejudice"'}, 'time': 25826.842757940292, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What is the capital city of France?', 'target_new': 'Lyon', 'ground_truth': 'Paris', 'portability': {}, 'locality': {}, 'subject': 'France'}, 'time': 5.254081726074219, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What instrument did Ludwig van Beethoven play?', 'target_new': 'Violin', 'ground_truth': 'Piano', 'portability': {}, 'locality': {}, 'subject': 'Ludwig van Beethoven'}, 'time': 4.407331705093384, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_QLoRA_llama3.2-3b.ipynb:244:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_QLoRA_llama3.2-3b.ipynb:247:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_QLoRA_llama3.2-3b.ipynb:250:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_QLoRA_llama3.2-3b.ipynb:253:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_QLoRA_llama3.2-3b.ipynb:256:      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_QLoRA_llama3.2-3b.ipynb:259:      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_IKE_InternLM.ipynb:693:      " {'pre': {'rewrite_acc': 0.3333333432674408, 'locality': {}, 'portability': {}, 'rephrase_acc': 0.6666666865348816}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Q: The president of the US is? A:', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'president', 'rephrase_prompt': 'The leader of the United State is'}, 'post': {'rewrite_acc': 0.6666666865348816, 'locality': {}, 'portability': {}, 'rephrase_acc': 0.6666666865348816}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_IKE_InternLM.ipynb:696:      " {'pre': {'rewrite_acc': 0.3333333432674408, 'locality': {}, 'portability': {}, 'rephrase_acc': 0.6666666865348816}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Q: The president of the US is? A:', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'president', 'rephrase_prompt': 'The leader of the United State is'}, 'post': {'rewrite_acc': 0.6666666865348816, 'locality': {}, 'portability': {}, 'rephrase_acc': 0.6666666865348816}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_IKE_InternLM.ipynb:705:      "[{'pre': {'rewrite_acc': 0.3333333432674408, 'locality': {}, 'portability': {}, 'rephrase_acc': 0.6666666865348816}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Q: The president of the US is? A:', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'president', 'rephrase_prompt': 'The leader of the United State is'}, 'post': {'rewrite_acc': 0.6666666865348816, 'locality': {}, 'portability': {}, 'rephrase_acc': 0.6666666865348816}}]n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_MEMIT_llama.ipynb:446:      " {'case_id': 0, 'time': 5.7448647022247314, 'post': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_MEMIT_llama.ipynb:448:      " {'case_id': 0, 'time': 5.7448647022247314, 'post': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_MEMIT_llama.ipynb:531:      " {'case_id': 1, 'time': 3.9600253105163574, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_MEMIT_llama.ipynb:533:      " {'case_id': 1, 'time': 3.9600253105163574, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_MEMIT_llama.ipynb:616:      " {'case_id': 2, 'time': 3.9899964332580566, 'post': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_MEMIT_llama.ipynb:618:      " {'case_id': 2, 'time': 3.9899964332580566, 'post': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_MEMIT_llama.ipynb:627:      "[{'case_id': 0, 'time': 5.7448647022247314, 'post': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}, {'case_id': 1, 'time': 3.9600253105163574, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}, {'case_id': 2, 'time': 3.9899964332580566, 'post': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_IKE.ipynb:1329:      " {'pre': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}, 'rephrase_acc': 1.0}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Q: The president of the US is? A:', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'president', 'rephrase_prompt': 'The leader of the United State is'}, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}, 'rephrase_acc': 1.0}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_IKE.ipynb:1332:      " {'pre': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}, 'rephrase_acc': 1.0}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Q: The president of the US is? A:', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'president', 'rephrase_prompt': 'The leader of the United State is'}, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}, 'rephrase_acc': 1.0}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_IKE.ipynb:1340:      "[{'pre': {'rewrite_acc': 0.5, 'locality': {}, 'portability': {}, 'rephrase_acc': 1.0}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Q: The president of the US is? A:', 'target_new': 'Joe Biden', 'ground_truth': 'Donald Trump', 'portability': {}, 'locality': {}, 'subject': 'president', 'rephrase_prompt': 'The leader of the United State is'}, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}, 'rephrase_acc': 1.0}}]n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_IKE.ipynb:1476:      "[{'case_id': 0, 'time': 0.25104260444641113, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}, 'rephrase_acc': 1.0}, 'pre': {'rewrite_acc': 0.5, 'portability': {}, 'rephrase_acc': 1.0}}]n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/.ipynb_checkpoints/EasyEdit_Example_ROME_llama-checkpoint.ipynb:273:      " {'case_id': 0, 'time': 4.404031991958618, 'post': {'rewrite_acc': 0.75, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/.ipynb_checkpoints/EasyEdit_Example_ROME_llama-checkpoint.ipynb:275:      " {'case_id': 0, 'time': 4.404031991958618, 'post': {'rewrite_acc': 0.75, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/.ipynb_checkpoints/EasyEdit_Example_ROME_llama-checkpoint.ipynb:329:      " {'case_id': 1, 'time': 3.9102234840393066, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/.ipynb_checkpoints/EasyEdit_Example_ROME_llama-checkpoint.ipynb:331:      " {'case_id': 1, 'time': 3.9102234840393066, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/.ipynb_checkpoints/EasyEdit_Example_ROME_llama-checkpoint.ipynb:385:      " {'case_id': 2, 'time': 3.9200351238250732, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/.ipynb_checkpoints/EasyEdit_Example_ROME_llama-checkpoint.ipynb:387:      " {'case_id': 2, 'time': 3.9200351238250732, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/.ipynb_checkpoints/EasyEdit_Example_ROME_llama-checkpoint.ipynb:402:      "[{'case_id': 0, 'time': 4.404031991958618, 'post': {'rewrite_acc': 0.75, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}, {'case_id': 1, 'time': 3.9102234840393066, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}, {'case_id': 2, 'time': 3.9200351238250732, 'post': {'rewrite_acc': 1.0, 'locality': {}, 'portability': {}}, 'pre': {'rewrite_acc': 0.0, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_InternLM.ipynb:792:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_InternLM.ipynb:795:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_InternLM.ipynb:818:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_InternLM.ipynb:821:      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_InternLM.ipynb:824:      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_InternLM.ipynb:827:      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}n"
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/tutorial-notebooks/EasyEdit_Example_ROME_InternLM.ipynb:835:      "[{'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}, {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]n",
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/models/melo/peft_egg/docs/source/task_guides/semantic_segmentation_lora.mdx:349:We can see that the LoRA-only parameters are just **2.2 MB in size**! This greatly improves the portability when using very large models.
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/models/melo/peft_egg/docs/source/task_guides/image_classification_lora.mdx:364:You'll see that it's only 2.6 MB! This greatly helps with portability, especially when using a very large model to fine-tune (such as [BLOOM](https://huggingface.co/bigscience/bloom)).
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:47:    total_score = score['Edit_acc']['final_score'] * 0.2 + score['portability']['final_score'] * 0.35 + score['locality']['final_score']  * 0.35 + score['fluency'] * 0.1
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:49:    result['scoreJson'] = {'score': total_score, 'Edit_acc':score['Edit_acc']['final_score'], 'portability':score['portability']['final_score'], 'locality':score['locality']['final_score'], 'fluency':score['fluency']}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:62:        elif(list(list2)!=['rewrite_ans','rephrase_ans','locality_ans','portability_ans'] and list(list2)!=['rewrite_ans','rephrase_ans','portability_ans']):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:65:        elif(list(list3)!=['prompt', 'target_new', 'ground_truth', 'portability', 'locality', 'subject','rephrase_prompt']):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:68:        elif(list(list4)!=['rewrite_ans','rephrase_ans','locality_ans','portability_ans','fluency'] and list(list4)!=['rewrite_ans','rephrase_ans','portability_ans','fluency']):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:135:    #evaluate portability
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:136:    portability_answer = []
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:137:    portability_outputs = []
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:139:        for an in item['requested_rewrite']['portability']['por_hop']['ground_truth']:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:140:            portability_answer.append(an)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:141:        for ou in item['post']['portability_ans']:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:142:            portability_outputs.append(ou)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate_cknowedit.py:143:    metrics['portability'] = compute_acc(portability_answer,portability_outputs)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:70:    ret['portability'] = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:84:    if 'portability' in record.keys() and any(record['portability']):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:85:        for portability_key in record['portability'].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:86:            ret['portability'].update(
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:87:                compute_portability_quality(model, model_name, hparams, tok, portability_key,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:88:                                            record['portability'][portability_key]['prompt'],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:89:                                            record['portability'][portability_key]['ground_truth'], device=device)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:193:def compute_portability_quality(
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:198:    portability_key: str,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:205:        portability_correct = test_prediction_acc_LLM_judge(model, tok, hparams, prompt, ground_truth, device, locality=False)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:207:        portability_correct = test_prediction_acc_LLM_judge(model, tok, hparams, prompt, ground_truth, device, locality=False)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:210:            portability_correct = test_seq2seq_batch_prediction_acc(model, tok, hparams, prompt, ground_truth, device)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:212:            portability_correct = test_prediction_acc(model, tok, hparams, prompt, ground_truth, device, vanilla_generation=hparams.alg_name=='GRACE')
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:215:        f"{portability_key}_acc": portability_correct
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:304:    if 'portability' in record.keys() and any(record['portability']):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:306:        for portability_key in record['portability'].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:309:            x = [f"{x_prefix}{x_p}" for x_p in record['portability'][portability_key]['prompt']]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:310:            acc, contain_score, gen_content = icl_lm_eval(model, model_name, hparams, tok, icl_input, record['portability'][portability_key]['ground_truth'],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:313:                f"{portability_key}_acc": acc,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:314:                f"{portability_key}_contain_acc": contain_score,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:315:                f"{portability_key}_gen_content": gen_content,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:317:            ret['portability'].update(key_ret)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:366:#     ret['portability'] = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:415:#     if 'portability' in record.keys() and any(record['portability']):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:416:#         for portability_key in record['portability'].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:423:#             if isinstance(record['portability'][portability_key]['ground_truth'], list):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:424:#                 portability_acc = []
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:425:#                 for x_a, x_p in zip(record['portability'][portability_key]['ground_truth'],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:426:#                                     record['portability'][portability_key]['prompt']):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:427:#                     tmp_portability_acc = icl_lm_eval(model, model_name, hparams, tok, icl_input, x_a,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:429:#                 portability_acc.append(tmp_portability_acc)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:431:#                 portability_acc = icl_lm_eval(model, model_name, hparams, tok, icl_input,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:432:#                                               record['portability'][portability_key]['ground_truth'],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:433:#                                               f"{x_prefix}{record['portability'][portability_key]['prompt']}")
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/evaluate/evaluate.py:434:#             ret['portability'][f'{portability_key}_acc'] = portability_acc
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:44:        for key in ["locality", "portability"]:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:95:        for key in ["locality", "portability"]:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:135:                      portability_inputs: Optional[Dict] = None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:143:        'portability': {},
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:218:    if portability_inputs is not None:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:219:        for portability_key in portability_inputs.keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:220:            if isinstance(portability_inputs[portability_key]['prompt'], str):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:221:                portability_inputs[portability_key]['prompt'] = [portability_inputs[portability_key]['prompt'],]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:222:                portability_inputs[portability_key]['ground_truth'] = [portability_inputs[portability_key]['ground_truth'], ]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:223:            assert len(portability_inputs[portability_key]['prompt']) == len(portability_inputs[portability_key]['ground_truth']) ./EasyEdit/easyeditor/editors/utils.py:224:            == len(requests), 'One Edit instance needs one portability input.....'
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:227:                if portability_inputs[portability_key]['prompt'][i] is not None:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:228:                    request['portability'].update(
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:230:                            portability_key: {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:231:                                'prompt': portability_inputs[portability_key]['prompt'][i],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/utils.py:232:                                'ground_truth': portability_inputs[portability_key]['ground_truth'][i]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:154:             portability_inputs: Optional[Dict] = None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:185:            requests = _prepare_requests(prompts, target_new, ground_truth, target_neg, rephrase_prompts, locality_inputs, portability_inputs, **kwargs)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:196:                   portability_inputs: Optional[Dict] = None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:220:        requests = _prepare_requests(prompts, target_new, ground_truth, target_neg, rephrase_prompts, locality_inputs, portability_inputs, **kwargs)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:539:        portability_inputs: Optional[Dict] = None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:555:            requests = _prepare_requests(prompts, target_new, ground_truth, target_neg, rephrase_prompts, locality_inputs, portability_inputs, **kwargs)
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:597:                for pr in request['portability']['por_hop']['prompt']:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:604:                results['pre']['portability_ans'] = por_results
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:647:                for pr in request['portability']['por_hop']['prompt']:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/editors/editor.py:654:                results_post['portability_ans'] = por_results
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/Cknowedit.py:76:                    "portability": record["portability"] if "portability" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/Cknowedit.py:150:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/Cknowedit.py:220:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/knowedit.py:70:                    "portability_r": record["portability"]["Reasoning"] if "portability" in record and "Reasoning" in record["portability"] else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/knowedit.py:71:                    "portability_s": record["portability"]["Subject_Aliasing"] if "portability" in record and "Subject_Aliasing" in record["portability"] else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/knowedit.py:72:                    "portability_l":record["portability"]["Logical_Generalization"] if "portability" in record and "Logical_Generalization" in record["portability"] else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/knowedit.py:146:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/knowedit.py:216:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/zsre.py:176:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/zsre.py:285:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:98:            # we set portability_metrics to None if not provided
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:99:            "portability_personas": record["personas"] if "personas" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:100:            "portability_hop": record["mhop"] if "mhop" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:101:            "portability_hop_ans":record["mhop_ans"] if "mhop_ans" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:122:            # we set portability_metrics to None if not provided
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:123:            "portability_personas": record["personas"] if "personas" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:124:            "portability_hop": record["mhop"] if "mhop" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:125:            "portability_hop_ans":record["mhop_ans"] if "mhop_ans" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:211:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/LEME.py:321:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:152:            if "portability" in record.keys() and record["portability"]:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:153:                request["portability"] = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:154:                request["portability"]["prompt"] = []
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:155:                request["portability"]["ground_truth"] = []
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:156:                for portability_key in record["portability"].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:159:                    if isinstance(record["portability"][portability_key], list):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:160:                        for item in record["portability"][portability_key]:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:163:                        request["portability"]["prompt"] += prompt
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:164:                        request["portability"]["ground_truth"] += ground_truth
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:166:                        request["portability"]["prompt"] += record["portability"][portability_key]["prompt"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:167:                        request["portability"]["ground_truth"] += record["portability"][portability_key]["ground_truth"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:289:        # portability
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:291:        if "portability" in batch[0].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:296:                port += b["portability"]["prompt"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/multitask.py:297:                port_ans += [' ' + i for i in b["portability"]["ground_truth"]]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/safety.py:151:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/safety.py:222:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/wikibigedit.py:71:                    "portability_personas": record["personas"] if "personas" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/wikibigedit.py:72:                    "portability_hop": record["mhop"] if "mhop" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/wikibigedit.py:73:                    "portability_hop_ans":record["mhop_ans"] if "mhop_ans" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/wikibigedit.py:147:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/wikibigedit.py:217:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/wiki_recent.py:189:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/counterfact.py:129:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/counterfact.py:227:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:164:            "portability_prompt": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:165:            "portability_ground_truth": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:178:            "portability_prompt": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:179:            "portability_ground_truth": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:193:            "portability_prompt": record["questions"],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:194:            "portability_ground_truth": [record["new_answer"]]*len(record["questions"]),
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:383:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/AKEW_both.py:453:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/MQuAKE.py:83:                    "portability_prompt": record["questions"],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/MQuAKE.py:84:                    "portability_ground_truth": [record["new_answer"]]*len(record["questions"]),
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/MQuAKE.py:164:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset_original/MQuAKE.py:273:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/Cknowedit.py:63:                    "portability": record["portability"] if "portability" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/Cknowedit.py:137:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/Cknowedit.py:207:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/HalluEditBench.py:131:                "portability": {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/knowedit.py:96:                    "portability_r": record["portability"]["Reasoning"] if "portability" in record and "Reasoning" in record["portability"] else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/knowedit.py:97:                    "portability_s": record["portability"]["Subject_Aliasing"] if "portability" in record and "Subject_Aliasing" in record["portability"] else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/knowedit.py:98:                    "portability_l":record["portability"]["Logical_Generalization"] if "portability" in record and "Logical_Generalization" in record["portability"] else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/knowedit.py:179:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/knowedit.py:249:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/zsre.py:176:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/zsre.py:285:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:98:            # we set portability_metrics to None if not provided
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:99:            "portability_personas": record["personas"] if "personas" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:100:            "portability_hop": record["mhop"] if "mhop" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:101:            "portability_hop_ans":record["mhop_ans"] if "mhop_ans" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:122:            # we set portability_metrics to None if not provided
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:123:            "portability_personas": record["personas"] if "personas" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:124:            "portability_hop": record["mhop"] if "mhop" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:125:            "portability_hop_ans":record["mhop_ans"] if "mhop_ans" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:211:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/LEME.py:321:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:152:            if "portability" in record.keys() and record["portability"]:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:153:                request["portability"] = {}
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:154:                request["portability"]["prompt"] = []
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:155:                request["portability"]["ground_truth"] = []
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:156:                for portability_key in record["portability"].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:159:                    if isinstance(record["portability"][portability_key], list):
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:160:                        for item in record["portability"][portability_key]:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:163:                        request["portability"]["prompt"] += prompt
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:164:                        request["portability"]["ground_truth"] += ground_truth
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:166:                        request["portability"]["prompt"] += record["portability"][portability_key]["prompt"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:167:                        request["portability"]["ground_truth"] += record["portability"][portability_key]["ground_truth"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:289:        # portability
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:291:        if "portability" in batch[0].keys():
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:296:                port += b["portability"]["prompt"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/multitask.py:297:                port_ans += [' ' + i for i in b["portability"]["ground_truth"]]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/safety.py:151:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/safety.py:222:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wikibigedit.py:71:                    "portability_personas": record["personas"] if "personas" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wikibigedit.py:72:                    "portability_hop": record["mhop"] if "mhop" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wikibigedit.py:73:                    "portability_hop_ans":record["mhop_ans"] if "mhop_ans" in record else None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wikibigedit.py:147:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wikibigedit.py:217:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_recent.py:189:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:77:            portability_prompt = None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:78:            portability_keys = list(record.get("portability", {}).keys())  # 获取 portability 中的所有键
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:79:            if portability_keys:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:80:                random_key = random.choice(portability_keys)  # 随机选择一个键
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:81:                if isinstance(record["portability"][random_key], list) and len(record["portability"][random_key]) > 0:
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:82:                    portability_prompt = record["portability"][random_key][0]["prompt"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:83:                    portability_ground_truth=record["portability"][random_key][0]["ground_truth"]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:101:                    "portability_prompt": portability_prompt,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:102:                    "portability_ground_truth": portability_ground_truth,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:187:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/wiki_counterfact.py:296:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/counterfact.py:129:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/counterfact.py:227:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:164:            "portability_prompt": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:165:            "portability_ground_truth": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:178:            "portability_prompt": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:179:            "portability_ground_truth": None,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:193:            "portability_prompt": record["questions"],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:194:            "portability_ground_truth": [record["new_answer"]]*len(record["questions"]),
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:383:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/AKEW_both.py:453:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/MQuAKE.py:83:                    "portability_prompt": record["questions"],
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/MQuAKE.py:84:                    "portability_ground_truth": [record["new_answer"]]*len(record["questions"]),
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/MQuAKE.py:164:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/easyeditor/dataset/MQuAKE.py:273:        # portability TODO
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:239:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:258:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:296:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:316:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:354:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:373:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:616:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:635:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:670:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:689:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:724:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:743:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:935:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:954:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:993:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1012:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1050:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1069:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1147:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1166:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1373:    portability_prompts = [edit_data_['portability']['New Question'] for edit_data_ in edit_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1374:    portability_ans = [edit_data_['portability']['New Answer'] for edit_data_ in edit_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1382:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1384:            'prompt': portability_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1385:            'ground_truth': portability_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1409:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1510:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1529:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1583:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1602:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1661:    portability_prompts = [edit_data_['portability']['New Question'] for edit_data_ in edit_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1662:    portability_ans = [edit_data_['portability']['New Answer'] for edit_data_ in edit_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1670:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1672:            'prompt': portability_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1673:            'ground_truth': portability_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1694:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1738:    portability_inputs = None
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1765:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1903:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1922:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1962:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:1981:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2148:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2169:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2231:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2250:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2373:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2395:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2486:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2506:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2641:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2662:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2785:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2804:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2907:    portability_prompts = [edit_data_['portability']['New Question'] for edit_data_ in edit_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2908:    portability_ans = [edit_data_['portability']['New Answer'] for edit_data_ in edit_data]
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2916:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2918:            'prompt': portability_prompts,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2919:            'ground_truth': portability_ans
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2932:        portability_inputs=portability_inputs,
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:2987:    portability_inputs = {
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:3002:    # 执行编辑，传递 locality 和 portability 输入
/home/wyren/Knowledge-Editing-Benchmark/EasyEdit/edit.py:3010:        portability_inputs=portability_inputs,
